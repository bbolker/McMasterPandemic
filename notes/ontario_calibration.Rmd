---
title: "analysis of Ontario COVID19 data"
author: "Ben Bolker, Michael Li, Jonathan Dushoff, David Earn (McMaster University)"
date: "`r format(Sys.time(),'%d %b %Y')`"
---

```{r opts, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,error=TRUE)
```

```{r pkgs,message=FALSE}
library(tidyverse)
library(glmmTMB)
library(broom.mixed)
library(McMasterPandemic)
library(ggplot2); theme_set(theme_bw())
library(colorspace)
library(corrplot)
## make nice defaults
scale_colour_discrete <- function(...) {
  colorspace::scale_colour_discrete_qualitative(...)
}
scale_fill_discrete <- function(...) {
  colorspace::scale_fill_discrete_qualitative(...)
}
```

```{r get_data, message=FALSE, warning=FALSE}
source("ontario_clean.R")
keep_vars <- c("H","ICU","D","report", "incidence")
ont_dd2 <- (ont_recent
    %>% mutate_at("var",trans_state_vars)
    %>% filter(var %in% keep_vars)
)
```

Reading data from Michael Li's [curated Canadian COVID data repository](https://wzmli.github.io/COVID19-Canada)
harvested from official sources such as [this](https://www.ontario.ca/page/2019-novel-coronavirus#section-0) and [hospitalization cumulative counts](http://wzmli.github.io/COVID19-Canada/PHO.csv) gathered from [Public Health Ontario Epidemiological Summary](https://files.ontario.ca/moh-covid-19-report-en-2020-04-07.pdf). (Latest data in this example are from `r format(max(ont_dd$date))`.)



```{r ONplot,message=FALSE, warning=FALSE}
print(gg1 <- ggplot(ont_dd,aes(date,value,colour=var))
  + geom_point()
  + scale_y_log10()
  + geom_smooth(method="lm",formula=y~poly(x,2))
)
```

- Natural for ICU and Ventilator to be parallel (no real lags here)
- recent nonlinearity/flattening in cases is too recent/sharp to affect the estimated slope much
- Weird that the ICU/vent curves are flattening before the other two: not apparently a capacity issue, maybe something about reporting (e.g. see [here](https://www.ontario.ca/page/2019-novel-coronavirus#7))?

Picture is cleaner (and flattening is more apparent) if we focus on more recent data.

```{r plot_recent, warning=FALSE}
print(gg1 %+% ont_recent)
```

We want to find the first _useful/meaningful_ day for each variable and adjust the different time series to their respective meaningful day.

```{r ont_plot_shift, message=FALSE, warning=FALSE}
print(gg2 <- ggplot(ont_recent,aes(vday,value,colour=var))
    + geom_point()
    + scale_y_log10()
    + geom_smooth(method="lm",formula=y~poly(x,2))
    + scale_x_continuous(limits=c(-1,NA))
)
```

## Fitting to data

We don't use this for calibration any more, but it is of some interest to fit log-quadratic curves to the data (this closely parallels the well-known IHME model ...)

```{r nbfit1, message=FALSE, warning=FALSE}
fit1 <- glmmTMB(value~var-1 + var:vday
  , family=nbinom2
  , dispformula=~var-1
  , data=ont_recent
)
```


Use raw polynomials to parameterize in terms of initial slope rather than average slope.

```{r nbfit_quad, message=FALSE, warning=FALSE}
fit1Q <- update(fit1, . ~ -1 + var + var:poly(vday,2,raw=TRUE))
```

"False convergence" warnings ... (can these be resolved?)


```{r tidying_fits, message=FALSE, warning=FALSE}
fix_term <- function(x) {
  gsub(":poly(vday, 2, raw = TRUE)","Poly"
    , gsub("var","",x)
    , fixed=TRUE
  )
}

t0 <- (tidy(fit1Q,conf.int=TRUE)
  %>% mutate_at("term",fix_term)
  %>% mutate(type=ifelse(grepl("[[:alpha:]]$",term)
          , "int"
          , ifelse(grepl("1$",term),"linear","quad")
          )
      , var=gsub("Poly[0-9]","",term)
      , var=factor(var,levels=rev(
          c("newConfirmations","Hospitalization","ICU","Ventilator","deceased")
          ))
      )
  %>% select(-c(effect,component,statistic,p.value))
)

```

Coefficient plot with tidied frame

```{r coefplot, warning=FALSE, message=FALSE}
print(gg3 <- ggplot(t0,aes(y=var,x=estimate,xmin=conf.low,xmax=conf.high))
    + geom_pointrange()
    + facet_wrap(~type,scale="free",ncol=1)
    + geom_vline(xintercept=0,lty=2)
    + labs(y="")
)
```

- intercepts for variables other than newConfirmations describe sensitivity (how small a non-zero value is actually reported?
- newConfirmations and hosp slopes and curvature agree fairly well
- don't know why death is slower  and ICU/Ventilator are faster (MLi: under reporting? there was a discussion on missing a lot of deaths. BMB: Underreporting alone doesn't change $r$ ... need some time-varying change in reporting rate etc..)
- deaths conf intervals might be overly narrow because these are cumulative values

## time-series calibration

We don't do this any more, but one calibration strategy is to use a phenomenological fit to the initial stage of the epidemic to estimate the initial growth rate ($r_0$).

We have a variety of time series (case reports, hospitalization, ICU, ventilator, death) from which to try to estimate initial growth rate $r_0$ (and potentially any time variation in $r$, which we would usually attribute to behavioural change or physical distancing efforts); we could pick any one of these, or some average.

* in general it is better to fit to incidence-like variables rather than cumulative values or prevalence (e.g. hospital/ICU/ventilation utilization), because cumulative and prevalence values are considerably autocorrelated. We can always make a cumulative variable into an incidence-like variable by differencing it, although in the case of health utilization (hospital, ICU, vent), this will include changes due to both admission and discharge; we would prefer a hospital admission time series if it were available. In general cumulative curves will still give reasonable estimates of the initial growth rate, but they will underestimate uncertainty [REF Rohani and King?]
* case confirmations are difficult to interpret because they are strongly confounded with time-varying testing intensity and criteria.
* when possible, especially when estimating *changes* in $r$ we would prefer to use leading rather than lagging indicators (cases > hospital admissions > hospital occupancy > death)
* in general more severe outcomes are less likely to be underreported or inconsistently reported (deaths > hospitalizations > infections)
* given that health utilization is not too high (saturating/overwhelming resources) ICU, vent, and hospital occupancy are relatively equivalent, given that a reasonably constant fraction of hospitalized cases require ICU/ventilators. (ICU occupancy will probably lag acute-care a bit because the average occupancy time is longer, although there is also a post-ICU acute-care period.)
* we chose to calibrate to hospitalizations, although as shown above there is not a big difference between the estimated $r$ based on new confirmations vs. hospital occupancy. 

This earlier version of calibration

* take a multivariate Normal sample of the intercept and initial slope from the log-quadratic fit; append a Normal sample from a prior for the generation interval (we used mean=6, sd=0.2 [probably too narrow])? For each sample we 

1. calibrated the parameters to $\bar G$ (adjusting latent period and infectious period for asymptomatic, mild, severe cases [not pre-symptomatic period])
2. calibrated the initial growth rate $r(0)$ to the slope (adjusting baseline transmission `beta0`)3. calibrate the starting value by picking $E(0)$ values until, starting from an earlier time with $S=N-E(0)$, $E=E(0)$ and all other states 0, we hit the intercept 
4. run the simulation forward.

Notes: (a) the first two steps are independent; (b) we could also calibrate the shape of the generation interval; (c) this calibration procedure doesn't account for later changes, and relies on estimates in the exponential phase; (d) it would be nice to replace step 3 with an analytical calculation based on the dominant eigenvector, but a combination of Jacobian issues and sensitivity to time steps etc. has made this hard so far.

## combined calibration

What if we want to estimate `beta0`, `E0`, and more than one breakpoint at the same time? 

Break points

- [March 17th closures](https://news.ontario.ca/opo/en/2020/03/ontario-enacts-declaration-of-emergency-to-protect-the-public.html)
- [March 23rd closure](https://news.ontario.ca/opo/en/2020/03/ontario-closing-at-risk-workplaces-to-protect-health-and-safety.html)
- [March 23rd closures](https://news.ontario.ca/opo/en/2020/03/ontario-orders-the-mandatory-closure-of-all-non-essential-workplaces-to-fight-spread-of-covid-19.html)
- [March 28th prohibits gathering](https://news.ontario.ca/opo/en/2020/03/ontario-prohibits-gatherings-of-five-people-or-more-with-strict-exceptions.html)

```{r cali_setup, message=FALSE, warning=FALSE}
params <- fix_pars(read_params("ICU1.csv")
    , target=c(Gbar=6)
    , pars_adj=list(c("sigma","gamma_s","gamma_m","gamma_a"))
)
params[["N"]] <- 19.5e6  ## reset pop to Ontario
summary(params)

schoolClose <- "17-Mar-2020"
countryClose <- "23-Mar-2020"
socialClose <- "28-Mar-2020"

bd <- ldmy(c(schoolClose,countryClose,socialClose))
print(bd)
```

**FIXME**: make this and cache it!
```{r cali_fit, message=FALSE,warning=FALSE}
opt_pars <- list(
    ## these params go to run_sim
    params=c(log_E0=4
             , log_beta0=-1
             ## fraction of mild (non-hosp) cases
             , log_mu=log(params[["mu"]])
             ## fraction of incidence reported
             ## logit_c_prop=qlogis(params[["c_prop"]]),
             ## fraction of hosp to acute (non-ICU)
             , logit_phi1=qlogis(params[["phi1"]])
             ## fraction of ICU cases dying
             ## logit_phi2=qlogis(params[["phi2"]])
             ),
    log_rel_beta0 = rep(-1, length(bd)),
    log_nb_disp=0)
t1 <- system.time(g1 <- calibrate(data=ont_recent
    , base_params=params
    , optim_args=list(control=list(maxit=10000),hessian=TRUE)
    , opt_pars = opt_pars,
    , break_dates = bd
    ## , debug=TRUE
      )
      ) ## system.time
```

The fit takes `r round(t1[["elapsed"]])` seconds.

Initially tried to fit `c_prop` (proportion of cases reported) and `phi2` (proportion of ICU dying) but got ridiculous answers and non-pos-def Hessians: real non-identifiability or just wonky data? Or some outcome of some of the other shortcuts/problems in the fitting (e.g. equal dispersion parameters per variable)?

```{r pars}
invlink_trans(restore(g1$par,opt_pars))
```

```{r}
vv <- solve(g1$hessian)
corrplot::corrplot.mixed(cov2cor(vv),lower="number",upper="ellipse")
```

Now forecast using the fitted parameters.

```{r rerun_forecast, message=FALSE, warning=FALSE}
f_args <-attr(g1,"forecast_args")
r <- do.call(forecast_sim,
    c(list(p=g1$par), f_args))
## FIXME: r can't use plot.pansim method ATM
rs <- dplyr::filter(r, var %in% keep_vars)
print(ggplot(rs,aes(date,value,colour=var))
      + geom_line()
      + scale_y_log10(limits=c(1,NA),oob=scales::squish)
      + geom_point(data=ont_dd2)
      + geom_vline(xintercept=ldmy(f_args$break_dates),lty=2)
      )
```

### forecast farther ahead


```{r forecast_ensemble2}
f_args2 <- f_args
f_args2$end_date <- "15-05-2020"
e_res2 <- forecast_ensemble(g1,
                           forecast_args=f_args2) %>% filter(var %in% keep_vars)
print(ggplot(e_res2, aes(date,value,colour=var,fill=var))
      + geom_line()
      ## + geom_ribbon(colour=NA, alpha=0.2, aes(ymin=lwr, ymax=upr))
      + geom_point(data=ont_dd2)
      + geom_vline(xintercept=ldmy(f_args$break_dates)-1,lty=2)
      + scale_y_log10(limits=c(1,NA), oob=scales::squish)
      )
```

```{r}
moment_params <- eval(formals(fix_pars)$pars_adj)[[2]]
out <- (describe_params(params_fitted)
    %>% mutate(type=case_when(
                   symbol %in% names(i1$params) ~ "mle-calibrated",
                   symbol %in% moment_params ~ "Gbar-calibrated",
                   TRUE ~ "assumed"),
               type=factor(type,levels=c("mle-calibrated",
                                         "Gbar-calibrated",
                                         "assumed")))
    %>% arrange(type)
)
```

What are the implied R0 values here?

```{r}
summary2 <- function(fit) {
    f_args <- attr(fit,"forecast_args")
    pars <- invlink_trans(restore(fit$par,f_args$opt_pars,f_args$fixed_pars))
    beta0 <- pars$params[["beta0"]]
    pp[[1]] <- update(f_args$base_params,beta0=beta0)
    for (i in seq_along(f_args$break_dates)) {
        pp[[i+1]] <- update(pp[[1]],
                            beta0=beta0*pars$rel_beta0[i])
    }
    ## r_jac <- suppressWarnings(vapply(pp,get_r,method="analytic",numeric(1)))
    names(pp) <- c(format(f_args$start_date),format(f_args$break_dates))
    ret <- purrr::map_dfr(pp,~as.data.frame(rbind(summary(.))),.id="start_date")
    return(ret)
}
print(ss <- summary2(g1))
saveRDS(ss,file="git_push/ontario_R0_cal1.rds")
```

## next

1. better/different approaches to estimating time-varying beta?




